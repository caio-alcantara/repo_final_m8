"""
Processador de HTML para Web Scraping do Inteli
Extrai conte√∫do estruturado mantendo hierarquia sem√¢ntica
Remove ru√≠do (navega√ß√£o, scripts, footers, etc)
"""

import re
from pathlib import Path
from typing import Any, Dict, List

from bs4 import BeautifulSoup, NavigableString, Tag


class InteliHTMLProcessor:
    """
    Processador especializado para HTMLs do site do Inteli.
    Mant√©m hierarquia sem√¢ntica enquanto remove ru√≠do.
    """

    # Tags sem√¢nticas que queremos preservar
    SEMANTIC_TAGS = ["article", "section", "main", "div"]

    # Tags de conte√∫do
    CONTENT_TAGS = ["p", "h1", "h2", "h3", "h4", "h5", "h6", "li", "blockquote", "pre"]

    # Tags/IDs/Classes para REMOVER (ru√≠do de navega√ß√£o)
    NOISE_PATTERNS = {
        "ids": [
            "header",
            "nav",
            "navigation",
            "menu",
            "sidebar",
            "footer",
            "comments",
            "social",
            "share",
            "related",
            "cookie",
            "banner",
            "ads",
            "advertisement",
        ],
        "classes": [
            "nav",
            "menu",
            "header",
            "footer",
            "sidebar",
            "share",
            "social",
            "comment",
            "related",
            "cookie",
            "banner",
            "breadcrumb",
            "pagination",
            "author-box",
            "meta",
            "tags",
            "categories",
            "newsletter",
            "subscribe",
        ],
        "tags": [
            "nav",
            "aside",
            "script",
            "style",
            "noscript",
            "iframe",
            "form",
            "button",
            "input",
        ],
    }

    # Padr√µes de texto para remover
    TEXT_NOISE_PATTERNS = [
        r"skip to content",
        r"acompanhe seu processo",
        r"quem somos",
        r"fundadores",
        r"campus",
        r"docentes",
        r"programa de bolsas",
        r"blog",
        r"contato",
        r"cursos",
        r"gradua√ß√£o",
        r"ensino b√°sico",
        r"educa√ß√£o executiva",
        r"seja um parceiro",
        r"doadores",
        r"redes sociais",
        r"acesse j√°",
        r"localiza√ß√£o",
        r"copyright",
        r"todos os direitos reservados",
        r"compartilhe:?",
        r"veja tamb√©m:?",
    ]

    def __init__(self):
        self.noise_pattern = re.compile(
            "|".join(self.TEXT_NOISE_PATTERNS), re.IGNORECASE
        )

    def is_noise_element(self, element: Tag) -> bool:
        """Detecta se um elemento √© ru√≠do (navega√ß√£o, menu, etc)."""
        if not isinstance(element, Tag):
            return False

        # Prote√ß√£o contra elementos com attrs=None
        if not hasattr(element, "attrs") or element.attrs is None:
            return False

        # Remove por tag
        if element.name in self.NOISE_PATTERNS["tags"]:
            return True

        # Remove por ID (com prote√ß√£o)
        try:
            elem_id = element.get("id", "").lower() if element.get("id") else ""
        except (AttributeError, TypeError):
            elem_id = ""

        if elem_id and any(noise in elem_id for noise in self.NOISE_PATTERNS["ids"]):
            return True

        # Remove por classe (com prote√ß√£o)
        try:
            elem_classes = (
                " ".join(element.get("class", [])).lower()
                if element.get("class")
                else ""
            )
        except (AttributeError, TypeError):
            elem_classes = ""

        if elem_classes and any(
            noise in elem_classes for noise in self.NOISE_PATTERNS["classes"]
        ):
            return True

        # Remove elementos vazios ou muito pequenos
        try:
            text = element.get_text(strip=True)
        except (AttributeError, TypeError):
            return True

        if len(text) < 10:
            return True

        # Remove por padr√£o de texto (menu items)
        if len(text) < 50 and self.noise_pattern.search(text):
            return True

        return False

    def extract_hierarchy(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """
        Extrai conte√∫do mantendo hierarquia sem√¢ntica.
        Retorna lista de elementos com contexto hier√°rquico.
        """
        elements = []

        # Usa o body direto (mais seguro para HTMLs complexos)
        main_content = soup.find("body")

        if not main_content:
            print("‚ö†Ô∏è Nenhum <body> encontrado no HTML")
            return elements

        print("‚úÖ Processando <body> completo")

        # Remove elementos de ru√≠do (com tratamento de erro)
        try:
            tags_to_remove = []
            all_tags = list(main_content.find_all())
            print(f"üìä Total de tags encontradas no body: {len(all_tags)}")

            for tag in all_tags:
                try:
                    if self.is_noise_element(tag):
                        tags_to_remove.append(tag)
                except Exception:
                    # Se der erro ao verificar, ignora o elemento
                    continue

            print(f"üóëÔ∏è Tags marcadas para remo√ß√£o (ru√≠do): {len(tags_to_remove)}")

            # Remove os tags identificados
            for tag in tags_to_remove:
                try:
                    tag.decompose()
                except Exception:
                    continue
        except Exception as e:
            # Se falhar completamente, continua sem remover ru√≠do
            print(f"‚ö†Ô∏è Aviso: Erro ao remover ru√≠do, continuando: {e}")

        # Tracking de contexto hier√°rquico
        hierarchy_stack = []
        current_section = "Introdu√ß√£o"

        # Contadores para debug
        processed_count = 0
        extracted_count = 0

        # Extrai elementos estruturados
        for element in main_content.descendants:
            try:
                processed_count += 1

                if isinstance(element, NavigableString):
                    continue

                if not isinstance(element, Tag):
                    continue

                tag_name = element.name
                if not tag_name:
                    continue

                # T√çTULOS (Hierarquia)
                if tag_name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
                    try:
                        text = element.get_text(strip=True)
                    except (AttributeError, TypeError):
                        continue

                    if len(text) < 5:
                        continue

                    # Detecta n√≠vel hier√°rquico
                    level = int(tag_name[1])  # h1 -> 1, h2 -> 2, etc

                    # Atualiza stack de hierarquia
                    while hierarchy_stack and hierarchy_stack[-1]["level"] >= level:
                        hierarchy_stack.pop()

                    hierarchy_stack.append({"level": level, "title": text})

                    current_section = text

                    elements.append(
                        {
                            "text": text,
                            "type": "Title",
                            "hierarchy_level": f"level_{level}",
                            "context_section": current_section,
                            "parent_sections": [
                                h["title"] for h in hierarchy_stack[:-1]
                            ],
                        }
                    )

                # PAR√ÅGRAFOS
                elif tag_name == "p":
                    try:
                        text = element.get_text(strip=True)
                    except (AttributeError, TypeError):
                        continue

                    if len(text) < 20:  # Ignora par√°grafos muito curtos
                        continue

                    # Verifica se n√£o √© ru√≠do
                    if self.noise_pattern.search(text):
                        continue

                    elements.append(
                        {
                            "text": text,
                            "type": "NarrativeText",
                            "hierarchy_level": "body",
                            "context_section": current_section,
                            "parent_sections": [h["title"] for h in hierarchy_stack],
                        }
                    )

                # LISTAS
                elif tag_name == "li":
                    try:
                        text = element.get_text(strip=True)
                    except (AttributeError, TypeError):
                        continue

                    if len(text) < 10:
                        continue

                    elements.append(
                        {
                            "text": text,
                            "type": "ListItem",
                            "hierarchy_level": "list_item",
                            "context_section": current_section,
                            "parent_sections": [h["title"] for h in hierarchy_stack],
                        }
                    )

                # BLOCKQUOTES
                elif tag_name == "blockquote":
                    try:
                        text = element.get_text(strip=True)
                    except (AttributeError, TypeError):
                        continue

                    if len(text) < 15:
                        continue

                    elements.append(
                        {
                            "text": text,
                            "type": "Quote",
                            "hierarchy_level": "body",
                            "context_section": current_section,
                            "parent_sections": [h["title"] for h in hierarchy_stack],
                        }
                    )

            except Exception:
                # Se der qualquer erro ao processar um elemento, pula ele
                continue

        print(f"üìä Elementos processados: {processed_count}")
        print(f"‚úÖ Elementos extra√≠dos: {len(elements)}")

        # Aplicar consolida√ß√£o sem√¢ntica
        if elements:
            print("\nüîó Aplicando consolida√ß√£o sem√¢ntica...")
            elements = self.group_by_semantic_context(elements)

        return elements

    def group_by_semantic_context(
        self, elements: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Agrupa elementos por contexto sem√¢ntico para reduzir fragmenta√ß√£o.

        Estrat√©gias de agrupamento:
        1. Listas: Agrupa list_items consecutivos da mesma se√ß√£o
        2. Professores: Agrupa nome + descri√ß√£o (se aplic√°vel)
        3. Conte√∫do relacionado: Agrupa elementos da mesma se√ß√£o sem√¢ntica

        Args:
            elements: Lista de elementos extra√≠dos do HTML

        Returns:
            Lista de elementos consolidados
        """
        grouped = []
        i = 0

        while i < len(elements):
            current = elements[i]
            current_text = current.get("text", "")
            current_type = current.get("type", "")
            current_section = current.get("context_section", "")

            # ========================================
            # 1. CONSOLIDAR LISTAS CONSECUTIVAS
            # ========================================
            if current_type == "ListItem":
                # Coletar itens de lista consecutivos da mesma se√ß√£o
                list_items = [current_text]
                j = i + 1

                while j < len(elements):
                    next_elem = elements[j]

                    if (
                        next_elem.get("type") == "ListItem"
                        and next_elem.get("context_section") == current_section
                    ):
                        list_items.append(next_elem.get("text", ""))
                        j += 1
                    else:
                        break

                # Se consolidou mais de 1 item, criar bloco de lista
                if len(list_items) > 1:
                    consolidated_list = {
                        "text": "\n".join(f"‚Ä¢ {item}" for item in list_items),
                        "type": "list_block",
                        "hierarchy_level": "list",
                        "context_section": current_section,
                        "parent_sections": current.get("parent_sections", []),
                        "metadata": {
                            **current.get("metadata", {}),
                            "consolidated": True,
                            "list_items_count": len(list_items),
                        },
                    }

                    grouped.append(consolidated_list)
                    i = j
                    continue

            # ========================================
            # 2. CONSOLIDAR PROFESSORES/ENTIDADES
            # ========================================
            elif current_type == "NarrativeText" and i + 1 < len(elements):
                next_elem = elements[i + 1]
                next_text = next_elem.get("text", "")
                next_section = next_elem.get("context_section", "")
                next_type = next_elem.get("type", "")

                # Se ambos s√£o da mesma se√ß√£o e parecem ser nome + descri√ß√£o
                if (
                    next_section == current_section
                    and next_type == "NarrativeText"
                    and len(current_text) < 100  # Nome curto
                    and len(next_text) > 50
                ):  # Descri√ß√£o mais longa
                    # Mesclar nome + descri√ß√£o
                    consolidated_entity = {
                        "text": f"{current_text}\n{next_text}",
                        "type": "NarrativeText",
                        "hierarchy_level": current.get("hierarchy_level", "body"),
                        "context_section": current_section,
                        "parent_sections": current.get("parent_sections", []),
                        "metadata": {
                            **current.get("metadata", {}),
                            "consolidated": True,
                            "entity_type": "profile",
                        },
                    }

                    grouped.append(consolidated_entity)
                    i += 2  # Pular ambos
                    continue

            # ========================================
            # 3. ELEMENTO N√ÉO CONSOLIDADO
            # ========================================
            # Adicionar normalmente
            grouped.append(current)
            i += 1

        # Estat√≠sticas
        reduction = len(elements) - len(grouped)
        if reduction > 0:
            print("   ‚úÖ Consolida√ß√£o aplicada:")
            print(f"      ‚Ä¢ Elementos antes: {len(elements)}")
            print(f"      ‚Ä¢ Elementos depois: {len(grouped)}")
            print(
                f"      ‚Ä¢ Redu√ß√£o: {reduction} ({reduction / len(elements) * 100:.1f}%)"
            )

        return grouped

    def process_html_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        Processa um arquivo HTML e retorna elementos estruturados.
        """
        try:
            html_content = file_path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            # Fallback para outras encodings
            try:
                html_content = file_path.read_text(encoding="latin-1")
            except:
                print(f"‚ö†Ô∏è Erro ao ler {file_path.name}")
                return []

        # Parse com BeautifulSoup
        soup = BeautifulSoup(html_content, "html.parser")

        # Extrai elementos com hierarquia
        elements = self.extract_hierarchy(soup)

        # Adiciona metadados do arquivo
        for element in elements:
            element["metadata"] = {
                "source": file_path.name,
                "file_type": "html",
                "category": self._infer_category(file_path.name),
            }

        return elements

    def _infer_category(self, filename: str) -> str:
        """Infere categoria baseado no nome do arquivo."""
        fname = filename.lower()

        if "processo-seletivo" in fname or "edital" in fname:
            return "processo_seletivo"
        elif "graduacao" in fname or "curso" in fname:
            return "graduacao"
        elif "mercado-tech" in fname or "tecnologia" in fname:
            return "mercado_tech"
        elif "campus" in fname or "estrutura" in fname:
            return "campus"
        elif "faq" in fname or "pergunta" in fname:
            return "faq"
        elif "bolsa" in fname or "financiamento" in fname:
            return "bolsas"

        return "geral"

    def batch_process_directory(
        self, dir_path: Path
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Processa todos os HTMLs de um diret√≥rio.
        Retorna dicion√°rio com {filename: elements}.
        """
        results = {}

        html_files = list(dir_path.glob("*.html"))
        print(f"üìÇ Encontrados {len(html_files)} arquivos HTML")

        for html_file in html_files:
            print(f"‚öôÔ∏è Processando: {html_file.name}")
            elements = self.process_html_file(html_file)

            if elements:
                results[html_file.name] = elements
                print(f"   ‚úÖ {len(elements)} elementos extra√≠dos")
            else:
                print("   ‚ö†Ô∏è Nenhum elemento extra√≠do")

        return results


# ================= FUN√á√ïES AUXILIARES =================


def clean_extracted_text(text: str) -> str:
    """Limpa texto extra√≠do do HTML."""
    if not text:
        return ""

    # Remove espa√ßos m√∫ltiplos
    text = re.sub(r"\s+", " ", text)

    # Remove caracteres de controle
    text = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]", "", text)

    # Remove URLs isoladas
    text = re.sub(r"https?://\S+", "", text)

    # Normaliza aspas e travess√µes
    text = text.replace('"', '"').replace('"', '"')
    text = text.replace("‚Äì", "-").replace("‚Äî", "-")

    return text.strip()


def convert_html_to_pipeline_format(
    elements: List[Dict[str, Any]], source_file: str
) -> List[Dict[str, Any]]:
    """
    Converte elementos extra√≠dos do HTML para formato do pipeline.
    """
    pipeline_elements = []

    for idx, element in enumerate(elements):
        text = clean_extracted_text(element["text"])

        if len(text) < 10:
            continue

        pipeline_elements.append(
            {
                "text": text,
                "type": element["type"],
                "metadata": {
                    "source": source_file,
                    "page_number": 1,
                    "element_type": element["type"],
                    "hierarchy_level": element["hierarchy_level"],
                    "context_section": element["context_section"],
                    "context_header": element.get("parent_sections", [])[-1]
                    if element.get("parent_sections")
                    else "",
                    "category": element["metadata"]["category"],
                    "is_header": element["type"] == "Title",
                },
                "element_id": f"html_{idx}",
            }
        )

    return pipeline_elements


# ================= EXEMPLO DE USO =================

if __name__ == "__main__":
    import sys
    from pathlib import Path

    if len(sys.argv) < 2:
        print("Uso: python html_processor.py <caminho_html_ou_diretorio>")
        sys.exit(1)

    input_path = Path(sys.argv[1])
    processor = InteliHTMLProcessor()

    if input_path.is_file():
        # Processa arquivo √∫nico
        print(f"üìÑ Processando arquivo: {input_path.name}\n")
        elements = processor.process_html_file(input_path)

        print(f"\n‚úÖ Total de elementos extra√≠dos: {len(elements)}")
        print("\nüìä Distribui√ß√£o por tipo:")
        types_count = {}
        for el in elements:
            t = el["type"]
            types_count[t] = types_count.get(t, 0) + 1

        for t, count in sorted(types_count.items()):
            print(f"   {t}: {count}")

        # Exibe primeiros 3 elementos
        print("\nüìù Primeiros 3 elementos:")
        for el in elements[:3]:
            print(f"\n[{el['type']}] (N√≠vel: {el['hierarchy_level']})")
            print(f"Se√ß√£o: {el['context_section']}")
            print(f"Texto: {el['text'][:100]}...")

    elif input_path.is_dir():
        # Processa diret√≥rio
        print(f"üìÇ Processando diret√≥rio: {input_path}\n")
        results = processor.batch_process_directory(input_path)

        print(f"\n‚úÖ Total de arquivos processados: {len(results)}")

        total_elements = sum(len(els) for els in results.values())
        print(f"‚úÖ Total de elementos extra√≠dos: {total_elements}")

        # Estat√≠sticas
        print("\nüìä Top 10 arquivos com mais elementos:")
        sorted_files = sorted(results.items(), key=lambda x: len(x[1]), reverse=True)
        for fname, els in sorted_files[:10]:
            print(f"   {fname}: {len(els)} elementos")

    else:
        print(f"‚ùå Erro: {input_path} n√£o √© um arquivo ou diret√≥rio v√°lido")
        sys.exit(1)
